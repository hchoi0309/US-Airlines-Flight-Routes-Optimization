{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try training several models to predict the demand, i.e., the number of daily passengers, for different flight routes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import re\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from joblib import dump\n",
    "\n",
    "pd.set_option(\"future.no_silent_downcasting\", True) # Prevent silent data type changes during operations for future compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training models, we first transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>airport_1</th>\n",
       "      <th>airport_2</th>\n",
       "      <th>distance</th>\n",
       "      <th>state_1</th>\n",
       "      <th>city_1</th>\n",
       "      <th>state_2</th>\n",
       "      <th>city_2</th>\n",
       "      <th>population_1</th>\n",
       "      <th>density_1</th>\n",
       "      <th>population_2</th>\n",
       "      <th>density_2</th>\n",
       "      <th>lat_1</th>\n",
       "      <th>lon_1</th>\n",
       "      <th>lat_2</th>\n",
       "      <th>lon_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147546</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>ORD</td>\n",
       "      <td>RIC</td>\n",
       "      <td>642</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>VA</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>8497759.0</td>\n",
       "      <td>4614.5</td>\n",
       "      <td>1073223.0</td>\n",
       "      <td>1463.7</td>\n",
       "      <td>41.775002</td>\n",
       "      <td>-87.696388</td>\n",
       "      <td>37.540759</td>\n",
       "      <td>-77.433932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12123</th>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PDX</td>\n",
       "      <td>1626</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>OR</td>\n",
       "      <td>Portland</td>\n",
       "      <td>5830932.0</td>\n",
       "      <td>1478.7</td>\n",
       "      <td>2095808.0</td>\n",
       "      <td>1868.8</td>\n",
       "      <td>40.110860</td>\n",
       "      <td>-77.035636</td>\n",
       "      <td>45.516018</td>\n",
       "      <td>-122.681425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167944</th>\n",
       "      <td>1993</td>\n",
       "      <td>3</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>DCA</td>\n",
       "      <td>1670</td>\n",
       "      <td>NM</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>769986.0</td>\n",
       "      <td>1159.8</td>\n",
       "      <td>5116378.0</td>\n",
       "      <td>4235.7</td>\n",
       "      <td>35.084248</td>\n",
       "      <td>-106.649241</td>\n",
       "      <td>38.892062</td>\n",
       "      <td>-77.019912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71631</th>\n",
       "      <td>1996</td>\n",
       "      <td>3</td>\n",
       "      <td>BOS</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1609</td>\n",
       "      <td>MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>4328315.0</td>\n",
       "      <td>5319.0</td>\n",
       "      <td>5970127.0</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>42.358894</td>\n",
       "      <td>-71.056742</td>\n",
       "      <td>29.760803</td>\n",
       "      <td>-95.369506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66928</th>\n",
       "      <td>1999</td>\n",
       "      <td>3</td>\n",
       "      <td>DAL</td>\n",
       "      <td>JAN</td>\n",
       "      <td>408</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>MS</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>5830932.0</td>\n",
       "      <td>1478.7</td>\n",
       "      <td>335171.0</td>\n",
       "      <td>529.9</td>\n",
       "      <td>40.110860</td>\n",
       "      <td>-77.035636</td>\n",
       "      <td>30.325968</td>\n",
       "      <td>-81.656760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  quarter airport_1 airport_2  distance state_1       city_1  \\\n",
       "147546  2009        4       ORD       RIC       642      IL      Chicago   \n",
       "12123   2006        3       DAL       PDX      1626      TX       Dallas   \n",
       "167944  1993        3       ABQ       DCA      1670      NM  Albuquerque   \n",
       "71631   1996        3       BOS       IAH      1609      MA       Boston   \n",
       "66928   1999        3       DAL       JAN       408      TX       Dallas   \n",
       "\n",
       "       state_2      city_2  population_1  density_1  population_2  density_2  \\\n",
       "147546      VA    Richmond     8497759.0     4614.5     1073223.0     1463.7   \n",
       "12123       OR    Portland     5830932.0     1478.7     2095808.0     1868.8   \n",
       "167944      DC  Washington      769986.0     1159.8     5116378.0     4235.7   \n",
       "71631       TX     Houston     4328315.0     5319.0     5970127.0     1384.0   \n",
       "66928       MS     Jackson     5830932.0     1478.7      335171.0      529.9   \n",
       "\n",
       "            lat_1       lon_1      lat_2       lon_2  \n",
       "147546  41.775002  -87.696388  37.540759  -77.433932  \n",
       "12123   40.110860  -77.035636  45.516018 -122.681425  \n",
       "167944  35.084248 -106.649241  38.892062  -77.019912  \n",
       "71631   42.358894  -71.056742  29.760803  -95.369506  \n",
       "66928   40.110860  -77.035636  30.325968  -81.656760  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "df = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"daily_passengers\"]), df[\"daily_passengers\"], test_size=0.2, random_state=42)\n",
    "\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quarter        4\n",
       "city_1       138\n",
       "city_2       125\n",
       "airport_1    186\n",
       "airport_2    172\n",
       "state_1       43\n",
       "state_2       40\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_cols = [\"quarter\", \"city_1\", \"city_2\", \"airport_1\", \"airport_2\", \"state_1\", \"state_2\"]\n",
    "unique_vals = X_train[categorical_cols].nunique()\n",
    "display(unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that most categorical variables have high cardinality values. To avoid the curse of dimensionality and efficiently deal with missing values, we use target encoding to convert categorical variables to be numeric. One-hot encoding will still be used for the `quarter` column as it only has four different nominal values. Finally, the data will be standardized before being fitted to different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column groups\n",
    "numeric_cols = [\"year\", \"distance\", \"population_1\", \"density_1\", \"population_2\", \"density_2\", \"lat_1\", \"lon_1\", \"lat_2\", \"lon_2\"]\n",
    "low_cardinality_cols = [\"quarter\"]\n",
    "high_cardinality_cols = [\"city_1\", \"city_2\", \"airport_1\", \"airport_2\", \"state_1\", \"state_2\"]\n",
    "\n",
    "# Create a column transformer for categorical encodings\n",
    "categorical_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"onehot\", OneHotEncoder(sparse_output=False), low_cardinality_cols),\n",
    "        (\"target\", TargetEncoder(random_state=42, target_type=\"continuous\", smooth=50), high_cardinality_cols),\n",
    "        (\"passthrough\", \"passthrough\", numeric_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Create a full pipeline with StandardScaler applied after encoding\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", categorical_transformer),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_test_transformed = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter_1</th>\n",
       "      <th>quarter_2</th>\n",
       "      <th>quarter_3</th>\n",
       "      <th>quarter_4</th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>airport_1</th>\n",
       "      <th>airport_2</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "      <th>year</th>\n",
       "      <th>distance</th>\n",
       "      <th>population_1</th>\n",
       "      <th>density_1</th>\n",
       "      <th>population_2</th>\n",
       "      <th>density_2</th>\n",
       "      <th>lat_1</th>\n",
       "      <th>lon_1</th>\n",
       "      <th>lat_2</th>\n",
       "      <th>lon_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147546</th>\n",
       "      <td>-0.592028</td>\n",
       "      <td>-0.571486</td>\n",
       "      <td>-0.576259</td>\n",
       "      <td>1.755523</td>\n",
       "      <td>1.098253</td>\n",
       "      <td>-1.374191</td>\n",
       "      <td>1.717321</td>\n",
       "      <td>-0.924878</td>\n",
       "      <td>1.352323</td>\n",
       "      <td>-2.409933</td>\n",
       "      <td>0.054919</td>\n",
       "      <td>-0.779459</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.443474</td>\n",
       "      <td>-0.828721</td>\n",
       "      <td>-0.759556</td>\n",
       "      <td>0.713284</td>\n",
       "      <td>0.127335</td>\n",
       "      <td>-0.159076</td>\n",
       "      <td>0.817684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12123</th>\n",
       "      <td>-0.592028</td>\n",
       "      <td>-0.571486</td>\n",
       "      <td>1.735332</td>\n",
       "      <td>-0.569631</td>\n",
       "      <td>0.436024</td>\n",
       "      <td>-0.288640</td>\n",
       "      <td>-0.362460</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>-0.104484</td>\n",
       "      <td>-0.360314</td>\n",
       "      <td>-0.289830</td>\n",
       "      <td>0.620218</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>-0.606917</td>\n",
       "      <td>-0.662790</td>\n",
       "      <td>-0.642145</td>\n",
       "      <td>0.363414</td>\n",
       "      <td>0.816499</td>\n",
       "      <td>1.530408</td>\n",
       "      <td>-1.566068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167944</th>\n",
       "      <td>-0.592028</td>\n",
       "      <td>-0.571486</td>\n",
       "      <td>1.735332</td>\n",
       "      <td>-0.569631</td>\n",
       "      <td>-0.772136</td>\n",
       "      <td>-0.006742</td>\n",
       "      <td>-0.547475</td>\n",
       "      <td>0.268853</td>\n",
       "      <td>-0.928142</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>-1.783745</td>\n",
       "      <td>0.682805</td>\n",
       "      <td>-0.900841</td>\n",
       "      <td>-0.713738</td>\n",
       "      <td>-0.172653</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>-0.693383</td>\n",
       "      <td>-1.097871</td>\n",
       "      <td>0.127185</td>\n",
       "      <td>0.839496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71631</th>\n",
       "      <td>-0.592028</td>\n",
       "      <td>-0.571486</td>\n",
       "      <td>1.735332</td>\n",
       "      <td>-0.569631</td>\n",
       "      <td>-0.061102</td>\n",
       "      <td>-0.173849</td>\n",
       "      <td>1.471245</td>\n",
       "      <td>0.254787</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>-0.234217</td>\n",
       "      <td>-1.438995</td>\n",
       "      <td>0.596037</td>\n",
       "      <td>-0.255718</td>\n",
       "      <td>0.679458</td>\n",
       "      <td>-0.034118</td>\n",
       "      <td>-0.782655</td>\n",
       "      <td>0.836042</td>\n",
       "      <td>1.203004</td>\n",
       "      <td>-1.807187</td>\n",
       "      <td>-0.127207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66928</th>\n",
       "      <td>-0.592028</td>\n",
       "      <td>-0.571486</td>\n",
       "      <td>1.735332</td>\n",
       "      <td>-0.569631</td>\n",
       "      <td>0.455444</td>\n",
       "      <td>-1.228839</td>\n",
       "      <td>-0.375767</td>\n",
       "      <td>-0.823625</td>\n",
       "      <td>-0.081362</td>\n",
       "      <td>-1.803863</td>\n",
       "      <td>-1.094246</td>\n",
       "      <td>-1.112309</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>-0.606917</td>\n",
       "      <td>-0.948482</td>\n",
       "      <td>-1.030199</td>\n",
       "      <td>0.363414</td>\n",
       "      <td>0.816499</td>\n",
       "      <td>-1.687462</td>\n",
       "      <td>0.595215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        quarter_1  quarter_2  quarter_3  quarter_4    city_1    city_2  \\\n",
       "147546  -0.592028  -0.571486  -0.576259   1.755523  1.098253 -1.374191   \n",
       "12123   -0.592028  -0.571486   1.735332  -0.569631  0.436024 -0.288640   \n",
       "167944  -0.592028  -0.571486   1.735332  -0.569631 -0.772136 -0.006742   \n",
       "71631   -0.592028  -0.571486   1.735332  -0.569631 -0.061102 -0.173849   \n",
       "66928   -0.592028  -0.571486   1.735332  -0.569631  0.455444 -1.228839   \n",
       "\n",
       "        airport_1  airport_2   state_1   state_2      year  distance  \\\n",
       "147546   1.717321  -0.924878  1.352323 -2.409933  0.054919 -0.779459   \n",
       "12123   -0.362460   0.013739 -0.104484 -0.360314 -0.289830  0.620218   \n",
       "167944  -0.547475   0.268853 -0.928142  0.006757 -1.783745  0.682805   \n",
       "71631    1.471245   0.254787 -0.069415 -0.234217 -1.438995  0.596037   \n",
       "66928   -0.375767  -0.823625 -0.081362 -1.803863 -1.094246 -1.112309   \n",
       "\n",
       "        population_1  density_1  population_2  density_2     lat_1     lon_1  \\\n",
       "147546      0.500200   0.443474     -0.828721  -0.759556  0.713284  0.127335   \n",
       "12123       0.016706  -0.606917     -0.662790  -0.642145  0.363414  0.816499   \n",
       "167944     -0.900841  -0.713738     -0.172653   0.043854 -0.693383 -1.097871   \n",
       "71631      -0.255718   0.679458     -0.034118  -0.782655  0.836042  1.203004   \n",
       "66928       0.016706  -0.606917     -0.948482  -1.030199  0.363414  0.816499   \n",
       "\n",
       "           lat_2     lon_2  \n",
       "147546 -0.159076  0.817684  \n",
       "12123   1.530408 -1.566068  \n",
       "167944  0.127185  0.839496  \n",
       "71631  -1.807187 -0.127207  \n",
       "66928  -1.687462  0.595215  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create feature names\n",
    "feature_names = ([f\"quarter_{i}\" for i in range(1, 5)] + high_cardinality_cols + numeric_cols)\n",
    "X_train = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pipeline and the feature names for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/feature_names.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(preprocessing_pipeline, \"../models/preprocessing_pipeline.joblib\")\n",
    "dump(feature_names, \"../models/feature_names.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression with L1 Regularization:\n",
    "<ul>\n",
    "  <li>\n",
    "    Higher interpretability: The results of a linear regression model is easier to interpret compared to other machine learning algorithms.\n",
    "  </li>\n",
    "  <li>\n",
    "    Feature Selection: LassoCV can be particularly valuable in reducing the number of features in a model by setting coefficients of less important features to zero.\n",
    "  </li>\n",
    "  <li>\n",
    "    Prevention of Overfitting: The L1 regularization helps prevent overfitting by constraining the size of the coefficients.\n",
    "  </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 144838.97372065447\n",
      "R2: 0.4368426132481338\n"
     ]
    }
   ],
   "source": [
    "lasso_cv = LassoCV(cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = lasso_cv.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:\n",
    "<ul>\n",
    "  <li>\n",
    "    Robustness: Random Forests are less prone to overfitting than many other algorithms because they average multiple deep decision trees, each trained on different parts of the same training set.\n",
    "  </li>\n",
    "  <li>\n",
    "    Handling of Non-linear Relationships: Random Forests can handle non-linear relationships between features and the target variable without the need for transformations.\n",
    "  </li>\n",
    "  <li>\n",
    "    Feature Handling: This algorithm can handle large data sets with higher dimensionality well; it can manage thousands of input variables without variable deletion, and it provides feature importance scores, which are helpful for understanding the factors driving predictions.\n",
    "  </li>\n",
    "  <li>\n",
    "    Versatility: Works well with both numerical and categorical data and does not require scaling of data.\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "Due to the size and complexity of our dataset, RandomizedSearchCV was used to search for the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  30.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  31.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  31.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  45.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  45.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  45.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  29.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  29.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  28.9s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  49.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  50.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  28.4s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  43.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  42.4s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  42.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  49.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  29.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  30.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  32.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  32.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  46.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  30.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  46.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  45.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  47.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  45.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  37.3s\n",
      "MSE: 20804.201805934426\n",
      "R2: 0.9191098941015367\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "param_distributions = {\n",
    "    'n_estimators': np.arange(100, 200, 300),\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'max_depth': [10, 30, 50, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=10, cv=3, verbose=2, n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = rf_search.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting:\n",
    "<ul>\n",
    "  <li>\n",
    "    Performance: Often provides superior predictive accuracy thanks to its efficient handling of various data types and distributions. XGBoost is known for winning many machine learning competitions due to its effectiveness.\n",
    "  </li>\n",
    "  <li>\n",
    "    Regularization: XGBoost includes built-in L1 and L2 regularization which helps prevent overfitting, which is crucial for high-dimensional data.\n",
    "  </li>\n",
    "  <li>\n",
    "    Scalability: XGBoost is designed to be highly efficient, scalable, and fast, even over large datasets.\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "Due to the size and complexity of our dataset, RandomizedSearchCV was used to search for the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   8.5s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   8.7s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.2s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.8s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.8; total time=  14.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.8; total time=  14.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.2s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.6; total time=  28.6s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.6; total time=  28.6s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.6; total time=  28.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=200, subsample=0.8; total time=  20.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=  12.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=200, subsample=0.8; total time=  22.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=200, subsample=0.8; total time=  22.2s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.6; total time=  16.4s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.6; total time=  15.4s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.6; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.6; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.6; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=6, n_estimators=300, subsample=0.6; total time=  14.2s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  24.8s\n",
      "MSE: 9774.944309346434\n",
      "R2: 0.9619934558868408\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor()\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions, n_iter=10, cv=3, verbose=2, n_jobs=-1, random_state=42)\n",
    "xgb_search.fit(X_train, y_train)\n",
    "y_test_pred = xgb_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 10371845120.0000 - val_loss: 7378892800.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 6527289856.0000 - val_loss: 6930135552.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - loss: 5995217408.0000 - val_loss: 6648556032.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5764955648.0000 - val_loss: 6409509376.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 6075241472.0000 - val_loss: 6233472512.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 5685865472.0000 - val_loss: 6110521344.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 5655907840.0000 - val_loss: 5999070720.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 5316113920.0000 - val_loss: 5938827264.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 5274073088.0000 - val_loss: 5877624320.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 4858282496.0000 - val_loss: 5815479296.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 5267309568.0000 - val_loss: 5778306560.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 5165391872.0000 - val_loss: 5751076352.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 5033792000.0000 - val_loss: 5721436160.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 5195048960.0000 - val_loss: 5690892800.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 5119696896.0000 - val_loss: 5651550720.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 5439769088.0000 - val_loss: 5651648000.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 5196641792.0000 - val_loss: 5605746688.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 4961946624.0000 - val_loss: 5587724288.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 4831471104.0000 - val_loss: 5555652096.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 5140891136.0000 - val_loss: 5541172224.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4818160640.0000 - val_loss: 5521296384.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4737073664.0000 - val_loss: 5511064064.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4906151936.0000 - val_loss: 5495628288.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 5171033600.0000 - val_loss: 5479878656.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 5071194112.0000 - val_loss: 5495563776.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4963837440.0000 - val_loss: 5462167040.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4777649664.0000 - val_loss: 5441470464.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4983944704.0000 - val_loss: 5433514496.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 4618896896.0000 - val_loss: 5421040640.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4802796032.0000 - val_loss: 5424463360.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4903891968.0000 - val_loss: 5419288576.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4932100608.0000 - val_loss: 5398805504.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4820709376.0000 - val_loss: 5402180608.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4808996864.0000 - val_loss: 5401538560.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4842248704.0000 - val_loss: 5386014208.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 5177167360.0000 - val_loss: 5381600256.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 5127689728.0000 - val_loss: 5378830848.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4794997760.0000 - val_loss: 5393153024.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4544956416.0000 - val_loss: 5354598400.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 4671923712.0000 - val_loss: 5342633472.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - loss: 4642714624.0000 - val_loss: 5349428736.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4776370176.0000 - val_loss: 5335833600.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - loss: 4764184576.0000 - val_loss: 5341883904.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4778120704.0000 - val_loss: 5317818880.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4933518848.0000 - val_loss: 5318329856.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 4827767296.0000 - val_loss: 5290035712.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 5137396736.0000 - val_loss: 5254225408.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 4844436992.0000 - val_loss: 5233760256.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4459833344.0000 - val_loss: 5202925568.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4995969024.0000 - val_loss: 5198438912.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4701911040.0000 - val_loss: 5132334592.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4789714944.0000 - val_loss: 5098442752.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4518495232.0000 - val_loss: 5085334528.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 4607463424.0000 - val_loss: 5037782528.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4539094528.0000 - val_loss: 5010052608.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4250102784.0000 - val_loss: 4982134784.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4437884416.0000 - val_loss: 4952147456.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4321680896.0000 - val_loss: 4923682816.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4656126976.0000 - val_loss: 4888480256.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4400423424.0000 - val_loss: 4865088512.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4389636096.0000 - val_loss: 4840655872.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4316080640.0000 - val_loss: 4809178112.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4294092288.0000 - val_loss: 4785243136.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4371296256.0000 - val_loss: 4760115200.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4402552320.0000 - val_loss: 4734510592.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4122021888.0000 - val_loss: 4710309888.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4527913472.0000 - val_loss: 4691978240.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 4434219520.0000 - val_loss: 4660276736.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4259432704.0000 - val_loss: 4640612864.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4069749504.0000 - val_loss: 4608763904.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4223334912.0000 - val_loss: 4598497792.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 4119236608.0000 - val_loss: 4596912640.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 4287753728.0000 - val_loss: 4582685696.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4019811584.0000 - val_loss: 4542457856.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 4039195392.0000 - val_loss: 4522978304.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 4125278720.0000 - val_loss: 4539203584.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3980305408.0000 - val_loss: 4487431680.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 3762117888.0000 - val_loss: 4488664576.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3857006080.0000 - val_loss: 4451317760.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 3944931584.0000 - val_loss: 4436545024.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 3857592064.0000 - val_loss: 4413744128.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 3847712512.0000 - val_loss: 4399755776.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3777662464.0000 - val_loss: 4421474304.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3705818880.0000 - val_loss: 4371837952.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3972739840.0000 - val_loss: 4352754688.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 3793482496.0000 - val_loss: 4339471360.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3872112640.0000 - val_loss: 4315354624.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3780380672.0000 - val_loss: 4308352000.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 3846799104.0000 - val_loss: 4295919104.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 3942927872.0000 - val_loss: 4275232256.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 3763634688.0000 - val_loss: 4256700672.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3578700800.0000 - val_loss: 4233554944.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 3772410112.0000 - val_loss: 4223200000.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 4040694528.0000 - val_loss: 4210204672.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 3871735808.0000 - val_loss: 4197454080.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3967257600.0000 - val_loss: 4184072192.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3602209792.0000 - val_loss: 4166644480.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 3727681792.0000 - val_loss: 4143623680.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 3630805248.0000 - val_loss: 4145230080.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m4920/4920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 3619828224.0000 - val_loss: 4129044992.0000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_encoded.shape[1],), kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set TensorFlow to only allocate memory as needed\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f'Using GPU: {gpus}')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1538/1538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "MSE: 4137766567.398363\n",
      "R2: 0.6764542284899852\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test).flatten()\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Gradient Boosting algorithm has performed the best on the test dataset, we will be using this model to predict the passenger demand for different flight routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/demand_prediction_model.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model for later use\n",
    "dump(xgb_search, \"../models/demand_prediction_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
